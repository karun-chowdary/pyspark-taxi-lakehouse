{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a080a38-8f5a-4f14-854b-039263cdb135",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Untitled"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType\n",
    "\n",
    "#intialising the SparkSession\n",
    "#using maxPartitionBytes helps us to control parallesiom. if not using this spark may read huge chunks of data and may cause outOfMemory(OOM)\n",
    "Spark = SparkSession.builder\\\n",
    "        .appName(\"NYC_Taxi_Analytics\")\\\n",
    "        .config(\"spark.sql.files.maxPartitionBytes\", \"128m\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cdc0ce31-f7b6-4942-9930-617f355ae573",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#defining the schema explicitly\n",
    "#It helps the spark not to read whole data for schema and helps in performance \n",
    "taxi_schema = StructType([\n",
    "    StructField(\"VendorID\", IntegerType(), True),\n",
    "    StructField(\"tpep_pickup_datetime\", TimestampType(), True),\n",
    "    StructField(\"tpep_dropoff_datetime\", TimestampType(), True),\n",
    "    StructField(\"passenger_count\", DoubleType(), True),\n",
    "    StructField(\"trip_distance\", DoubleType(), True),\n",
    "    StructField(\"PULocationID\", IntegerType(), True),\n",
    "    StructField(\"DOLocationID\", IntegerType(), True),\n",
    "    StructField(\"rateCodeId\", IntegerType(), True),\n",
    "    StructField(\"store_and_fwd_flag\", StringType(), True),\n",
    "    StructField(\"payment_type\", IntegerType(), True),\n",
    "    StructField(\"fare_amount\", DoubleType(), True),\n",
    "    StructField(\"extra\", DoubleType(), True),\n",
    "    StructField(\"mta_tax\", DoubleType(), True),\n",
    "    StructField(\"tip_amount\", DoubleType(), True),\n",
    "    StructField(\"tolls_amount\", DoubleType(), True),\n",
    "    StructField(\"improvement_surcharge\", DoubleType(), True),\n",
    "    StructField(\"total_amount\", DoubleType(), True),\n",
    "    StructField(\"congestion_surcharge\", DoubleType(), True)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c3815d7-17a8-42cd-8a37-6a77e322a79c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "taxi_df = spark.read\\\n",
    "    .schema(taxi_schema)\\\n",
    "        .option(\"mode\",\"DROPMALFORMED\")\\\n",
    "            .csv(\"dbfs:/databricks-datasets/nyctaxi/tripdata/yellow/yellow_tripdata_2019-01.csv.gz\")\n",
    "print(f\"Initial Count : {taxi_df.count()}\")\n",
    "\n",
    "#For df.count() spark doesnot pull all data at option. if it does it will crash memory. instead, it peforms a partial sum \n",
    "#As we mentioned maxPartiitonsBytes is 128m\n",
    "#Driver gives instrcution to executors to count rows in your slice.\n",
    "#After counting executors do not send all rows back to driver. it will send only that integer count to driver in his(executor) slice\n",
    "#In reading CSV. spark must read all data to find new line characters(\\n) to count rows.\n",
    "#When it is parquet file spark will skip data and reads the metadata at footer of file. it will complete it in milliseconds\n",
    "#As we are using \"DROPMALFORMED\" mode. it must physically scan the data to see if any rows are malformed (corrupt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e8e2a54-5fc3-4d2d-acee-748796a9e9b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, month, to_date, when, lit\n",
    "# Cleaning garbage data (Negative fares, 0 distance)\n",
    "# LOGIC: A trip with 0 distance but > $0 fare is likely valid (cancellation fee), but we exclude for this analysis.\n",
    "taxi_clean_df = taxi_df.filter(\n",
    "    (col('fare_amount')>0) & (col('trip_distance')>0)\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6fdf5910-45ca-4d7f-b61d-c8a6697c75f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#transformations on the data, calculating trip_duration_seconds, revenue_per_mile, is_high_value\n",
    "taxi_df_transformed = taxi_clean_df\\\n",
    "    .withColumn(\"trip_duration_seconds\",col('tpep_dropoff_datetime').cast('long') - col('tpep_pickup_datetime').cast('long'))\\\n",
    "        .withColumn(\"revenue_per_mile\",col('total_amount')/col('trip_distance'))\\\n",
    "            .withColumn(\"is_high_value\",when(col('total_amount')> 50, lit('HIGH')).otherwise(lit('STANDARD')))\n",
    "\n",
    "#Lazy evaluation : when we run above lines spark will build only logical map, it doesn't run any code.\n",
    "# Catalyst optimiser = when you do filter . spark sends your fare>0 to the file reader. it pulls only good rows into the memory. bad rows leave it harddrive.\n",
    "#whole_stage_code_generation = spark doesn't run line by line in python as it is slow. it compilrs 3-4 lines of python code into one single optimized java function(Bytecode). it eliminates the 'object overhead' of python and runs raw CPU instructions on binary data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "383eac33-f816-43ac-b7ff-f1cf2219875d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Broadcast join\n",
    "# Why? Zone names are small (<10MB). Broadcasting avoids Shuffling.\n",
    "zone_schema = \"LocationID INT, Borough STRING, zone STRING, service_zone STRING\"\n",
    "zones_df = spark.read.csv(\"dbfs:/databricks-datasets/nyctaxi/taxizone/taxi_zone_lookup.csv\", schema = zone_schema, header = True)\n",
    "taxi_df_transformed = taxi_df_transformed.withColumn(\"PULocationID\", col(\"PULocationID\").cast(\"int\"))\n",
    "zones_df = zones_df.withColumn(\"LocationID\", col(\"LocationID\").cast(\"int\"))\n",
    "\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "#joinging using broadcast\n",
    "taxi_joined_df = taxi_df_transformed.join(\n",
    "    broadcast(zones_df),\n",
    "    taxi_df_transformed.PULocationID == zones_df.LocationID,\n",
    "    \"left\").drop(\"LocationID\") # drop duplicate columns \n",
    "# 3. Debug: Check if we actually have matches now\n",
    "match_count = taxi_joined_df.filter(col(\"Borough\").isNotNull()).count()\n",
    "print(f\"Total matched rows: {match_count}\")\n",
    "\n",
    "# If this prints a number > 0, the data is definitely there!\n",
    "print(f\"Verified Records in Memory: {taxi_joined_df.limit(100).count()}\")\n",
    "\n",
    "# Selecting specific columns makes the display much lighter and faster\n",
    "display(taxi_joined_df.select(\"VendorID\", \"Borough\", \"total_amount\").limit(20))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80e8914b-4f49-42db-9d84-38dbd4e44365",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#finding top 5 Earning zones\n",
    "taxi_joined_df.groupby('zone')\\\n",
    "    .sum('total_amount')\\\n",
    "    .orderBy(col('sum(total_amount)').desc())\\\n",
    "    .show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97c55b8c-006f-4140-98ad-13a974a2cc68",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Pivot Table - Payment Type Analysis\n",
    "# Goal: See total tips by Payment Type (Credit vs Cash) per Boroug\n",
    "\n",
    "taxi_pivot_df = taxi_joined_df.groupBy(\"Borough\")\\\n",
    "    .pivot('payment_type',[1,2])\\\n",
    "        .sum('tip_amount')\n",
    "\n",
    "#After Pivot You have one row per Borough with columns\n",
    "#This is essential for converting data into a format ready for Dashboards (like Tableau/PowerBI) or Machine Learning feature vectors.\n",
    "#optimization: giving explicitly how many rows needed for pivot will decrease processing time 50%. if not it will scan entire data for distinct values it will take more time.\n",
    "# spark physically rewrites the code above into SQL internally(CASE WHEN + GROUP BY)\n",
    "'''SELECT \n",
    "  Borough,\n",
    "  SUM(CASE WHEN payment_type = 1 THEN tip_amount ELSE 0 END) as `1`,\n",
    "  SUM(CASE WHEN payment_type = 2 THEN tip_amount ELSE 0 END) as `2`\n",
    "FROM table\n",
    "GROUP BY Borough'''\n",
    "#This optimization (projection) happens in the Catalyst Optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78d3bec1-4069-455b-a14e-c8ee39b8a755",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Aggregating by vendor and borough and subtotals for each\n",
    "df = taxi_joined_df.rollup('vendorID','Borough').avg('total_amount').orderBy(\"vendorID\",\"Borough\")\n",
    "df.show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7b2b814-8707-4f14-a020-44b8abcde878",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import lag, col\n",
    "\n",
    "#defining window specification\n",
    "#group by vendorID and sot by timeline\n",
    "window_spec = Window.partitionBy(\"VendorID\").orderBy(\"tpep_pickup_datetime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05d9206d-cd4a-4504-95c7-85fd4b627897",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#using LAG to look back at previous rows drop off times\n",
    "window_df = taxi_joined_df.withColumn(\"prev_dropoff_time\",lag(\"tpep_dropoff_datetime\").over(window_spec)).withColumn(\"idle_time_seconds\",col(\"tpep_pickup_datetime\").cast('long') - col(\"prev_dropoff_time\").cast('long'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61e17c09-f464-4a81-830b-01db55c8d889",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import rank\n",
    "\n",
    "#defining window spec for the ranking\n",
    "#we partition by borough and sort by total_amount in descending order.\n",
    "rank_spec = Window.partitionBy(\"Borough\").orderBy(col('total_amount').desc())\n",
    "\n",
    "#apply rank and filter\n",
    "top_trips_df = taxi_joined_df.withColumn(\"Borough_rank\",rank().over(rank_spec)).filter(col(\"borough_rank\")<=3)\n",
    "# If this number is > 0, your data is physically present!\n",
    "\n",
    "display(top_trips_df.select(\"Borough\", \"total_amount\", \"Borough_rank\").limit(20))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Taxi trip record data",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
