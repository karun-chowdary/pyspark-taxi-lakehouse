{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a080a38-8f5a-4f14-854b-039263cdb135",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Untitled"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType\n",
    "\n",
    "#intialising the SparkSession\n",
    "#using maxPartitionBytes helps us to control parallesiom. if not using this spark may read huge chunks of data and may cause outOfMemory(OOM)\n",
    "Spark = SparkSession.builder\\\n",
    "        .appName(\"NYC_Taxi_Analytics\")\\\n",
    "        .config(\"spark.sql.files.maxPartitionBytes\", \"128m\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cdc0ce31-f7b6-4942-9930-617f355ae573",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#defining the schema explicitly\n",
    "#It helps the spark not to read whole data for schema and helps in performance \n",
    "taxi_schema = StructType([\n",
    "    StructField(\"VendorID\", IntegerType(), True),\n",
    "    StructField(\"tpep_pickup_datetime\", TimestampType(), True),\n",
    "    StructField(\"tpep_dropoff_datetime\", TimestampType(), True),\n",
    "    StructField(\"passenger_count\", DoubleType(), True),\n",
    "    StructField(\"trip_distance\", DoubleType(), True),\n",
    "    StructField(\"PULocationID\", IntegerType(), True),\n",
    "    StructField(\"DOLocationID\", IntegerType(), True),\n",
    "    StructField(\"rateCodeId\", IntegerType(), True),\n",
    "    StructField(\"store_and_fwd_flag\", StringType(), True),\n",
    "    StructField(\"payment_type\", IntegerType(), True),\n",
    "    StructField(\"fare_amount\", DoubleType(), True),\n",
    "    StructField(\"extra\", DoubleType(), True),\n",
    "    StructField(\"mta_tax\", DoubleType(), True),\n",
    "    StructField(\"tip_amount\", DoubleType(), True),\n",
    "    StructField(\"tolls_amount\", DoubleType(), True),\n",
    "    StructField(\"improvement_surcharge\", DoubleType(), True),\n",
    "    StructField(\"total_amount\", DoubleType(), True),\n",
    "    StructField(\"congestion_surcharge\", DoubleType(), True)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c3815d7-17a8-42cd-8a37-6a77e322a79c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "taxi_df = spark.read\\\n",
    "    .schema(taxi_schema)\\\n",
    "        .option(\"mode\",\"DROPMALFORMED\")\\\n",
    "            .csv(\"dbfs:/databricks-datasets/nyctaxi/tripdata/yellow/yellow_tripdata_2019-01.csv.gz\")\n",
    "print(f\"Initial Count : {taxi_df.count()}\")\n",
    "\n",
    "#For df.count() spark doesnot pull all data at option. if it does it will crash memory. instead, it peforms a partial sum \n",
    "#As we mentioned maxPartiitonsBytes is 128m\n",
    "#Driver gives instrcution to executors to count rows in your slice.\n",
    "#After counting executors do not send all rows back to driver. it will send only that integer count to driver in his(executor) slice\n",
    "#In reading CSV. spark must read all data to find new line characters(\\n) to count rows.\n",
    "#When it is parquet file spark will skip data and reads the metadata at footer of file. it will complete it in milliseconds\n",
    "#As we are using \"DROPMALFORMED\" mode. it must physically scan the data to see if any rows are malformed (corrupt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e8e2a54-5fc3-4d2d-acee-748796a9e9b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, month, to_date, when, lit\n",
    "# Cleaning garbage data (Negative fares, 0 distance)\n",
    "# LOGIC: A trip with 0 distance but > $0 fare is likely valid (cancellation fee), but we exclude for this analysis.\n",
    "taxi_clean_df = taxi_df.filter(\n",
    "    (col('fare_amount')>0) & (col('trip_distance')>0)\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6fdf5910-45ca-4d7f-b61d-c8a6697c75f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#transformations on the data, calculating trip_duration_seconds, revenue_per_mile, is_high_value\n",
    "taxi_df_transformed = taxi_clean_df\\\n",
    "    .withColumn(\"trip_duration_seconds\",col('tpep_dropoff_datetime').cast('long') - col('tpep_pickup_datetime').cast('long'))\\\n",
    "        .withColumn(\"revenue_per_mile\",col('total_amount')/col('trip_distance'))\\\n",
    "            .withColumn(\"is_high_value\",when(col('total_amount')> 50, lit('HIGH')).otherwise(lit('STANDARD')))\n",
    "\n",
    "#Lazy evaluation : when we run above lines spark will build only logical map, it doesn't run any code.\n",
    "# Catalyst optimiser = when you do filter . spark sends your fare>0 to the file reader. it pulls only good rows into the memory. bad rows leave it harddrive.\n",
    "#whole_stage_code_generation = spark doesn't run line by line in python as it is slow. it compilrs 3-4 lines of python code into one single optimized java function(Bytecode). it eliminates the 'object overhead' of python and runs raw CPU instructions on binary data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "383eac33-f816-43ac-b7ff-f1cf2219875d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Broadcast join\n",
    "# Why? Zone names are small (<10MB). Broadcasting avoids Shuffling.\n",
    "zone_schema = \"LocationID INT, Borough STRING, zone STRING, service_zone STRING\"\n",
    "zones_df = spark.read.csv(\"dbfs:/databricks-datasets/nyctaxi/taxizone/taxi_zone_lookup.csv\", schema = zone_schema, header = True)\n",
    "\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "#joinging using broadcast\n",
    "taxi_joined_df = taxi_df_transformed.join(\n",
    "    broadcast(zones_df),\n",
    "    taxi_df_transformed.PULocationID == zones_df.LocationID,\n",
    "    \"left\").drop(\"LocationID\") # drop duplicate columns "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Taxi trip record data",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
