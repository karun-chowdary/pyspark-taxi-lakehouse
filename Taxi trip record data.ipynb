{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a080a38-8f5a-4f14-854b-039263cdb135",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Untitled"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType\n",
    "\n",
    "#intialising the SparkSession\n",
    "#using maxPartitionBytes helps us to control parallesiom. if not using this spark may read huge chunks of data and may cause outOfMemory(OOM)\n",
    "Spark = SparkSession.builder\\\n",
    "        .appName(\"NYC_Taxi_Analytics\")\\\n",
    "        .config(\"spark.sql.files.maxPartitionBytes\", \"128m\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cdc0ce31-f7b6-4942-9930-617f355ae573",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#defining the schema explicitly\n",
    "#It helps the spark not to read whole data for schema and helps in performance \n",
    "taxi_schema = StructType([\n",
    "    StructField(\"VendorID\", IntegerType(), True),\n",
    "    StructField(\"tpep_pickup_datetime\", TimestampType(), True),\n",
    "    StructField(\"tpep_dropoff_datetime\", TimestampType(), True),\n",
    "    StructField(\"passenger_count\", DoubleType(), True),\n",
    "    StructField(\"trip_distance\", DoubleType(), True),\n",
    "    StructField(\"PULocationID\", IntegerType(), True),\n",
    "    StructField(\"DOLocationID\", IntegerType(), True),\n",
    "    StructField(\"rateCodeId\", IntegerType(), True),\n",
    "    StructField(\"store_and_fwd_flag\", StringType(), True),\n",
    "    StructField(\"payment_type\", IntegerType(), True),\n",
    "    StructField(\"fare_amount\", DoubleType(), True),\n",
    "    StructField(\"extra\", DoubleType(), True),\n",
    "    StructField(\"mta_tax\", DoubleType(), True),\n",
    "    StructField(\"tip_amount\", DoubleType(), True),\n",
    "    StructField(\"tolls_amount\", DoubleType(), True),\n",
    "    StructField(\"improvement_surcharge\", DoubleType(), True),\n",
    "    StructField(\"total_amount\", DoubleType(), True),\n",
    "    StructField(\"congestion_surcharge\", DoubleType(), True)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c3815d7-17a8-42cd-8a37-6a77e322a79c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "taxi_df = spark.read\\\n",
    "    .schema(taxi_schema)\\\n",
    "        .option(\"mode\",\"DROPMALFORMED\")\\\n",
    "            .csv(\"dbfs:/databricks-datasets/nyctaxi/tripdata/yellow/yellow_tripdata_2019-01.csv.gz\")\n",
    "print(f\"Initial Count : {taxi_df.count()}\")\n",
    "\n",
    "#For df.count() spark doesnot pull all data at option. if it does it will crash memory. instead, it peforms a partial sum \n",
    "#As we mentioned maxPartiitonsBytes is 128m\n",
    "#Driver gives instrcution to executors to count rows in your slice.\n",
    "#After counting executors do not send all rows back to driver. it will send only that integer count to driver in his(executor) slice\n",
    "#In reading CSV. spark must read all data to find new line characters(\\n) to count rows.\n",
    "#When it is parquet file spark will skip data and reads the metadata at footer of file. it will complete it in milliseconds\n",
    "#As we are using \"DROPMALFORMED\" mode. it must physically scan the data to see if any rows are malformed (corrupt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4e8e2a54-5fc3-4d2d-acee-748796a9e9b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Taxi trip record data",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
